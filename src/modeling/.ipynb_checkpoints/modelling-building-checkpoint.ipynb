{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2cb5e8-ac9b-4b9e-8176-c2811e12fd5d",
   "metadata": {},
   "source": [
    "### Modeling with UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edc8182a-0e9f-43c2-b026-e373d861893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "from unet import UNet \n",
    "from dataset import HotosmDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db199cfe-fd11-47fa-8955-fe1d46aef2af",
   "metadata": {},
   "source": [
    "### Training of the UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40ba191-d741-496d-9bf4-8d26751fe3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters here\n",
    "learning_rate = 3e-4\n",
    "batch_size = 8\n",
    "epochs = 10 # no. of epochs\n",
    "\n",
    "# Location of the images and mask\n",
    "data_path = \"/Users/savin/Omdena-Projects/HOTOSM/data/Mask/building_mask.npz\"\n",
    "\n",
    "# define the device  here\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2e8a3-d2ea-44ca-aeb8-78b4cf2b7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling dataset class\n",
    "# This is an effective and clean way to access data based on Pytorch\n",
    "# DataLoader class will utilize this custom dataset object to split data into batches, shuffle and \n",
    "# effective retrieval\n",
    "train_dataset = HotosmDataset(data_path)\n",
    "generator = torch.Generator().manual_seed(42) # setting up a manual seed for consistent results\n",
    "train_dataset, test_dataset = random_split(train_dataset, [0.8, 0.2], generator=generator)\n",
    "val_dataset, test_dataset = random_split(test_dataset, [0.5, 0.5], generator=generator)\n",
    "\n",
    "# calling the dataloader\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(len(train_dataloader), len(val_dataloader), len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f0f42-277e-4f44-b27c-d68d3a2a3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (img, mask) in enumerate(train_dataloader):\n",
    "    if i==4:\n",
    "        print(img.dtype, img.shape, mask.shape, mask.dtype)\n",
    "        print(np.unique(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948b343-34e7-4dae-800a-de4b28597b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HotosmDataset(data_path)\n",
    "img, mask = train_dataset.__getitem__(100)\n",
    "print(img.dtype, mask.dtype)\n",
    "print(img.shape, mask.shape)\n",
    "print(np.unique(mask))\n",
    "print(train_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f410c-9d74-4c38-a152-5adc320e93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(data_path)\n",
    "img = data['image']\n",
    "mask = data['mask']\n",
    "\n",
    "print(img.dtype, mask.dtype)\n",
    "print(np.unique(img[0]))\n",
    "print(np.unique(mask[0]))\n",
    "print(img.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ece58-99f1-4e90-8bac-319c7e98bbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors for each class\n",
    "cmap = colors.ListedColormap(['yellow', 'red', 'blue', 'black'])\n",
    "\n",
    "# Define the normalization boundaries\n",
    "bounds = [0, 1, 2, 3, 4]  # Upper bound is non-inclusive\n",
    "norm = colors.BoundaryNorm(bounds, cmap.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635c727-e6e8-4d83-bbe3-a029aa9f4b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation(images, masks):\n",
    "    fig, axs = plt.subplots(8, 2, figsize=(10, 16))\n",
    "    for i in range(8):\n",
    "        axs[i][0].imshow(images[i].permute(1, 2, 0))\n",
    "        axs[i][0].set_title(\"Input Image\")\n",
    "        #print(masks[i].unique())\n",
    "        axs[i][1].imshow(masks[i], cmap=cmap, norm=norm)\n",
    "        axs[i][1].set_title(\"Ground Truth\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for i, (images, masks) in tqdm(enumerate(train_dataloader)):\n",
    "    if i==0:\n",
    "        visualize_segmentation(images, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf9d1d9-b437-4592-9499-8c2eb2045fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the model\n",
    "# Ideally you want to load all the model and data to the device\n",
    "# There are 4 classes for building: Background, masonry, metal and others\n",
    "model = UNet(in_channels=3, num_classes=4).to(device) # defining the model\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate) # Adam version suited for transformers and UNET architectures.\n",
    "criterion = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3860a-83ee-4b69-a100-b6494966a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for e in range(epochs): # starting the training iterations\n",
    "    model.train() # setting the model to train mode\n",
    "    train_running_loss = 0 # calculate loss for each epoch\n",
    "    print(f\"calculating loss across {len(train_dataloader)} batches\")\n",
    "\n",
    "    for img, mask in tqdm(train_dataloader): # data contains the image and mask\n",
    "\n",
    "        #print(f\"Processing epoch:{e}, batch {num}\")\n",
    "        img = img.to(device) # each of this should be [batch_size, channels, height, width]\n",
    "        \n",
    "        mask = mask.to(device)\n",
    "\n",
    "        #print(img.shape, mask.shape)\n",
    "\n",
    "        ypred = model(img) # This basically outputs the logits and not probabilities. \n",
    "        # output shape:  [batch, classes, height, width]\n",
    "        # mask shape: [batch, height, width] == where pixel value is the index of each class\n",
    "        # A computational graph is created each during the feed forward network and remeber all the calculations\n",
    "        # Because CrossEntropyLoss internally does:\n",
    "\n",
    "        # 1. calculate p = -LogSoftmax(logits) for each pixel\n",
    "        # 2. loss of single pixel = p[class_index] or selecting -log(softmax(logits)) for the true class\n",
    "        # 4. Averaging over all the pixels of the image and the batch.\n",
    "        # 5. Basically generalize the normal multi-class classification along different pixels in an image.\n",
    "\n",
    "        optimizer.zero_grad() # Zero the gradients in each batch, otherwise they will accumulate\n",
    "\n",
    "        loss = criterion(ypred, mask) # ypred[nbatch, nclass, height, width], mask:[nbatch, height, width] pixel = 0,1,2...N classes\n",
    "        train_running_loss += loss.item()\n",
    "\n",
    "        loss.backward() # Back propogation step, calculate the gradient of each parameter and updates the parameter.grad field. \n",
    "        # This calculate gradient needs to be cleared after each batch, other wise it will start accumulating.\n",
    "        optimizer.step() # update the parameters using the calculated gradients in the .grad field of each parameter\n",
    "\n",
    "        # Delete unnecessary data to free up GPU memory\n",
    "        del img, mask, ypred, loss\n",
    "\n",
    "        # Empty unused cache\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "   \n",
    "    train_loss = train_running_loss/len(train_dataloader) # Average across batches\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Now set the model to the evaluation mode\n",
    "    model.eval()\n",
    "    val_running_loss = 0\n",
    "    with torch.no_grad(): # stop the backpropogation for the evaluation mode and no need to create the compuation graph in the forward run\n",
    "        for img, mask in tqdm(val_dataloader): # data contains the image and mask\n",
    "            img = img.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            ypred = model(img)\n",
    "            loss = criterion(ypred, mask)\n",
    "\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "            del img, mask, ypred\n",
    "\n",
    "            # Empty unused cache\n",
    "            torch.mps.empty_cache()\n",
    "   \n",
    "        \n",
    "        val_loss = val_running_loss/len(val_dataloader)\n",
    "        val_losses.append(val_loss)\n",
    "    # print statistics across for each epoch\n",
    "    print(\"*\"*20)\n",
    "    print(f\"Train loss, epoch {e} : {train_loss:0.4f}\")\n",
    "    print(f\"Validation loss, epoch {e} : {val_loss:0.4f}\")\n",
    "    print(\"*\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c097db8-b489-48c6-9680-41717759f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to\n",
    "model_save_path = \"models/\"\n",
    "\n",
    "# save the history as a data frame\n",
    "df = pd.DataFrame(data={\"train_loss\":train_losses, \"val_loss\":val_losses})\n",
    "df.to_csv(model_save_path+\"hist_building.csv\")\n",
    "\n",
    "#save pytorch model\n",
    "#torch.save(model.state_dict(),  model_save_path)\n",
    "# save to torchscript format, no model defining is needed\n",
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save(model_save_path+'unet_building.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449acb5-6125-4cdc-81ea-07a9c2c1b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.read_csv(\"models/hist_building.csv\")\n",
    "hist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054bb56-1ce6-4c70-a5c4-0e40df450e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = hist['train_loss']\n",
    "val_loss = hist['val_loss']\n",
    "\n",
    "n_epochs = len(train_loss)\n",
    "\n",
    "epochs_list = list(range(1, n_epochs + 1))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "plt.plot(epochs_list, train_loss, label='Training Loss')\n",
    "plt.plot(epochs_list, val_loss, label='Validation Loss')\n",
    "#plt.xticks(ticks=list(range(1, EPOCHS + 1, 1))) \n",
    "plt.title('Loss over epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399688b4-9891-4523-a373-04e70862451a",
   "metadata": {},
   "source": [
    "## Metric calculation\n",
    "Mainly 3 metrics can be used here:\n",
    "1. Dice score\n",
    "2. IOU\n",
    "3. Pixel accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252e830-f26d-4c44-9360-e94dd0a455ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the metrics\n",
    "\n",
    "def compute_multiclass_metrics(outputs, targets, num_classes, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Computes Dice, IoU, and Pixel Accuracy for multi-class segmentation.\n",
    "\n",
    "    Args:\n",
    "        outputs: model logits or softmax outputs, shape [B, C, H, W]\n",
    "        targets: ground truth, shape [B, H, W] with values 0...C-1\n",
    "    \"\"\"\n",
    "    preds = torch.argmax(outputs, dim=1)  # shape [B, H, W]\n",
    "\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    pixel_accs = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = (preds == cls).float()\n",
    "        target_cls = (targets == cls).float()\n",
    "\n",
    "        intersection = (pred_cls * target_cls).sum(dim=(1, 2))\n",
    "        union = pred_cls.sum(dim=(1, 2)) + target_cls.sum(dim=(1, 2))\n",
    "        dice = (2. * intersection + eps) / (union + eps)\n",
    "\n",
    "        iou = (intersection + eps) / (\n",
    "            pred_cls.sum(dim=(1, 2)) + target_cls.sum(dim=(1, 2)) - intersection + eps\n",
    "        )\n",
    "\n",
    "        dice_scores.append(dice.mean().item()) # average dice score of all images within a batch\n",
    "        iou_scores.append(iou.mean().item())  # average iou score of all images within a batch\n",
    "\n",
    "    # Pixel Accuracy over all images \n",
    "    pixel_acc = (preds == targets).float().mean().item() \n",
    "\n",
    "    return {\n",
    "        'mean_dice': sum(dice_scores) / num_classes,\n",
    "        'mean_iou': sum(iou_scores) / num_classes,\n",
    "        'pixel_accuracy': pixel_acc,\n",
    "        'per_class_dice': dice_scores,\n",
    "        'per_class_iou': iou_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59345d97-2011-4c4b-81aa-dad99317780b",
   "metadata": {},
   "source": [
    "#### Evaluation of the model on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b6107-2286-4b14-9baf-5adde08e0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.jit.load(\"models/unet_building.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111399fd-2a6d-4432-ad49-8c9cac4c8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_metrics_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(val_dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = model(images)  # Shape: [B, C, H, W]\n",
    "\n",
    "        metrics = compute_multiclass_metrics(outputs, masks, num_classes=4)\n",
    "        val_metrics_list.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2dac93-428d-4785-b963-1a84f0d94f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics_list):\n",
    "    # Average the metrics across all batches\n",
    "    avg_mean_dice = np.array([metric['mean_dice'] for metric in metrics_list]).mean()\n",
    "    avg_mean_iou = np.array([metric['mean_iou'] for metric in metrics_list]).mean()\n",
    "    avg_pix_accuracy = np.array([metric['pixel_accuracy'] for metric in metrics_list]).mean()\n",
    "    avg_per_class_dice = np.array([metric['per_class_dice'] for metric in metrics_list]).mean(axis=0)\n",
    "    avg_per_class_iou = np.array([metric['per_class_iou'] for metric in metrics_list]).mean(axis=0)\n",
    "    \n",
    "    print(\"Metric averaged over batches\")\n",
    "    print(f\"Dice (averaged over class) - batch average: {avg_mean_dice:.4f}\")\n",
    "    print(f\"IoU (averaged over class) - batch average: {avg_mean_iou:.4f}\")\n",
    "    print(f\"Pixel Accuracy - batch average : {avg_pix_accuracy:.4f}\")\n",
    "    print(f\"Per class dice - batch average: {np.round(avg_per_class_dice, 4)}\")\n",
    "    print(f\"Per class IoU - batch average: {np.round(avg_per_class_iou, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677c3ee-5219-4e51-a228-84376395178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(val_metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66779d39-a164-4095-b9a9-1de8317d52bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f5093bb-aa15-488e-bd37-e82d0db06b50",
   "metadata": {},
   "source": [
    "#### Evaluation of the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0cc88-16cb-4cd5-ae77-57760c8429b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_metrics_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = model(images)  # Shape: [B, C, H, W]\n",
    "\n",
    "        metrics = compute_multiclass_metrics(outputs, masks, num_classes=4)\n",
    "        test_metrics_list.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5dfc30-d8e8-4699-9d39-f928dfdb629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(test_metrics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070a198-4e51-41fb-a22f-514a12355849",
   "metadata": {},
   "source": [
    "The evaluation results look better on the test set compared to the validatation set. The DICE and IoU score is higher for the 'other' class and the background class. Let' visualize a the first batch of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be0c25-8c1d-413b-b24e-79fc617d56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_segmentation(images, masks, preds, class_names=None):\n",
    "    images = images.cpu()\n",
    "    masks = masks.cpu()\n",
    "    preds = preds.cpu()\n",
    "\n",
    "    fig, axs = plt.subplots(len(images), 3, figsize=(10, 3 * len(images)))\n",
    "    if len(images) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        axs[i][0].imshow(images[i].permute(1, 2, 0))\n",
    "        axs[i][0].set_title(\"Input Image\")\n",
    "        axs[i][1].imshow(masks[i], cmap=cmap, norm=norm)\n",
    "        axs[i][1].set_title(\"Ground Truth\")\n",
    "        axs[i][2].imshow(preds[i], cmap=cmap, norm=norm)\n",
    "        axs[i][2].set_title(\"Prediction\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, masks) in tqdm(enumerate(test_dataloader)):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = model(images)  # Shape: [B, C, H, W]\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        if i==2:\n",
    "            visualize_segmentation(images, masks, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff3f977-72d7-4def-90d9-ce34e931a317",
   "metadata": {},
   "source": [
    "## HOTOSM Road Classification\n",
    "In this notebook, we will try to classify the road conditions as paved and unpaved using the street level images collected from the\n",
    "Mapillary project.\n",
    "\n",
    "For the classification, we will train a convolutional neural network architecture called MobileNet which is a light weight model, usually deployed in edge devices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fd50850-9541-4347-9d09-9c1ba2674489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, Input, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6482d8-e7a0-4a2d-8f8c-9496f414c168",
   "metadata": {},
   "source": [
    "### Exploratory data analysis\n",
    "Let's take a look at the data first. The data for this road classification exist in 2 folders, paved and unpaved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77b53350-e20e-4c57-ace2-7835c2d995f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.jpg\n",
      "1.jpg\n",
      "10.jpg\n",
      "11.jpg\n",
      "12.jpg\n",
      "13.jpg\n",
      "14.jpg\n",
      "16.jpg\n",
      "17.jpg\n",
      "18.jpg\n"
     ]
    }
   ],
   "source": [
    "! ls /Users/savin/Omdena-Projects/HOTOSM/data/Roads-Classification/Paved/ | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f90e09c-87ac-46e4-a7f6-2e1e13f9a5cd",
   "metadata": {},
   "source": [
    "Now let's collect the filepath of files in each folder and their labels to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30933d35-62cb-4b07-bd07-77b617c19052",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/savin/Omdena-Projects/HOTOSM/data/Roads/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/savin/Omdena-Projects/HOTOSM/data/Roads/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Listing number of image files in each folder list\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m folds \u001b[38;5;241m=\u001b[39m [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m  \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(data_path\u001b[38;5;241m+\u001b[39mitem)] \u001b[38;5;66;03m# image directories\u001b[39;00m\n\u001b[1;32m      6\u001b[0m file_len \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(folds)) \u001b[38;5;66;03m# Number of files in each folder\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Collecting the filepath of each files and their associated folder labeling\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Will be used for train and test split of the data\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/savin/Omdena-Projects/HOTOSM/data/Roads/'"
     ]
    }
   ],
   "source": [
    "# Directory containing the data\n",
    "data_path = \"/Users/savin/Omdena-Projects/HOTOSM/data/Roads-Classification/\"\n",
    "\n",
    "# Listing number of image files in each folder list\n",
    "folds = [item for item in  os.listdir(data_path) if os.path.isdir(data_path+item)] # image directories\n",
    "file_len = np.zeros(len(folds)) # Number of files in each folder\n",
    "\n",
    "# Collecting the filepath of each files and their associated folder labeling\n",
    "# Will be used for train and test split of the data\n",
    "image_filepath = []\n",
    "labels = []\n",
    "\n",
    "for i,fold in enumerate(sorted(folds)):\n",
    "    files = glob.glob(data_path+fold+\"/*.jpg\")\n",
    "    nfiles = len(files)\n",
    "    file_len[i] = nfiles\n",
    "    for filepath in files:\n",
    "        image_filepath.append(filepath)\n",
    "        labels.append(i)\n",
    "\n",
    "print(f\"Number of files in {folds[0]} folder: {file_len[0]: 0.0f}\")\n",
    "print(f\"Number of files in {folds[1]} folder: {file_len[1]: 0.0f}\")\n",
    "\n",
    "# saving the information into a pandas dictionary\n",
    "df = pd.DataFrame.from_dict({'X_path':image_filepath, 'Y':labels})\n",
    "df = df.sample(frac=1) # shuffling the data\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.bar(sorted(folds), file_len)\n",
    "ax.set_ylabel(\"No. of files\")\n",
    "ax.tick_params(axis='x', labelrotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d71310-a002-466e-9eb5-0b4c477c6d6b",
   "metadata": {},
   "source": [
    "Now Let's read the images in these folders into a numpy array which we can use for modelling. But before that we need to make sure if all of these images have the same dimensions.\n",
    "\n",
    "#### Data Splitting to train and test\n",
    "Before loading the data, let's split the data into train and test. Since we have a limited amount of images, we are splitting into cross validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716e850-2d07-495d-9c45-5834ff830d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e87c37-d60b-46df-8280-32c6b7a2905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratisfied splitting the data into 80% train and 20 % test to ensure that the test data contains \n",
    "# more or less equal distributions of different classes\n",
    "X_train_path, X_test_path, Y_train, Y_test = train_test_split(df['X_path'], df['Y'], test_size=0.2, random_state=42, stratify = df['Y'])\n",
    "print(Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e3d76b-d74a-4da3-a658-7ebfca148207",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "We will try to get the metdata of the images to understand the dimension of the images. If they are different we need to resize them before feeding into CNN algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8444a-4f50-4113-979b-2a6f24cdd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to get the metadata information from the image files without reading them in memory using the Pillow package\n",
    "\n",
    "h = [] # height\n",
    "w = [] # width\n",
    "\n",
    "for i, filepath in enumerate(df['X_path']):\n",
    "    img = Image.open(filepath)\n",
    "    width, height = img.size\n",
    "    h.append(height)\n",
    "    w.append(width)\n",
    "\n",
    "print(np.unique(np.array(h)))\n",
    "print(np.unique(np.array(w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930a1ca-0a50-4213-8b03-f9aa29b58fc3",
   "metadata": {},
   "source": [
    "Interestingly, the dimensions of the images varies a lots as they are collected through different sources. Let's resize all the images to (224 by 160), which are the acceptable image dimensions of the DL models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4103f-869f-4e80-84d4-55ffdce3188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the image file into a numpy array\n",
    "#dimensions of the resized image\n",
    "width, height = [224, 160]\n",
    "\n",
    "def load_data(df):\n",
    "    \"\"\"\n",
    "    Returns the array with loaded images from\n",
    "    the image files\n",
    "    args:\n",
    "    df : dataframe containing the filepaths\n",
    "    \"\"\"\n",
    "    samp = df.shape[0]\n",
    "    #print(samp)\n",
    "    data = np.zeros((samp, height, width, 3), dtype = np.float32) # Each image with width, height and 3 RGB channels\n",
    "    for i in range(samp):\n",
    "        filename = df.iloc[i]\n",
    "        \n",
    "        # Load the image\n",
    "        img = cv2.imread(filename)\n",
    "        \n",
    "        if img is not None:\n",
    "            img_resized = cv2.resize(img, (width, height), interpolation=cv2.INTER_AREA) # resize the images\n",
    "            data[i,:,:,:] = img_resized/255 # Normalize the pixel values\n",
    "        else:\n",
    "            print(\"Image cannot be opened or read\")\n",
    "\n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f499ee-7812-4ebf-9065-fa61d42d0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train data and test data\n",
    "\n",
    "X_train = load_data(X_train_path)\n",
    "X_test = load_data(X_test_path)\n",
    "\n",
    "print(f\" X train shape : {X_train.shape}\")\n",
    "print(f\"X test shape : {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0579ff-aead-48ed-ab96-a4bbca92ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a random image files to see the collected data\n",
    "fig, axs = plt.subplots(4, 4, figsize=(12,10), tight_layout = True)\n",
    "axs = axs.flatten()\n",
    "for i in range(len(axs)):\n",
    "    axs[i].imshow(X_train[i, :, :, :])\n",
    "    axs[i].set_title(f\"Cat:{Y_train.iloc[i]}\")\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90989004-6beb-45f4-a517-8bff446c9490",
   "metadata": {},
   "source": [
    "The class: 0  is paved and class:1 is unpaved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d592d6f-1b0c-4543-a589-6bd132b7212d",
   "metadata": {},
   "source": [
    "## Modeling of data\n",
    "For modeling, we will try the MobileNetV2 model. This model utilizes a combination of the bottleneck units which uses a combination of expansion, depthwise seperated convolution and projection) along with residual units to improve the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e080f-457c-49f5-9e48-b88c4209dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tensorflow model here\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Let's create a function for the model\n",
    "\n",
    "def call_mobilenet(input_shape):\n",
    "    \n",
    "    # Calling the Resnet model\n",
    "    mob_model = MobileNetV2(\n",
    "    include_top=False, # remove the top layer or the classification layer (1000 classes from imagenet training)\n",
    "    weights=\"imagenet\", # utilize imagenet weights\n",
    "    input_tensor=None,\n",
    "    input_shape=input_shape, # time steps axis not needed here\n",
    "    pooling=None)  # no pooling mentioned\n",
    "\n",
    "    #print a summary of the base model\n",
    "    mob_model.summary()\n",
    "\n",
    "    nb_layers = len(mob_model.layers)\n",
    "    print(f\"No. of MobileNetV2 model layers: {nb_layers}\")\n",
    "\n",
    "    # Freeze the base model as we wan to use the pretrained weights\n",
    "    mob_model.trainable = False\n",
    "\n",
    "    return mob_model\n",
    "\n",
    "def create_model(input_shape, nclass):\n",
    "\n",
    "    \"\"\"\n",
    "    Function returns the model output\n",
    "    Args:\n",
    "    input shape: shape of the input train/test data (excluding the batch size)\n",
    "    nclass: number of output classes\n",
    "    \"\"\"\n",
    "    # start the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # defining the input layer\n",
    "    model.add(Input(shape=input_shape))\n",
    "\n",
    "    # resnet layer for feature extraction\n",
    "    model.add(call_mobilenet(input_shape)) # calling the mobilenet function here\n",
    "\n",
    "    # adding a 2D global average to reduce the size and summarize the info in each channel\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    \n",
    "    # flattening  not needed anymore with the global averaging\n",
    "    \n",
    "    # Drop out regularization\n",
    "    model.add(Dropout(0.2))\n",
    "              \n",
    "    # fully connected layers\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    \n",
    "    # Final output layer, Binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4fc8e3-1633-4d65-b3e6-3e8837bed8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nclass = len(np.unique(Y_train))\n",
    "input_shape = X_train.shape[1:] \n",
    "print(input_shape, nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684113ea-a201-4e55-9aaa-3b7f021b2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "mob_netv2 = create_model(input_shape, nclass)\n",
    "# This will print out the basic MobileNetV2 architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a8339-1ee5-4ae1-9c11-e8570e5e0820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the overall model summary\n",
    "mob_netv2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df94c97-daaa-4c6a-ad51-8fc07b151018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling process\n",
    "mob_netv2.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1ba2f2-2cd4-4a04-be47-9739e2fa101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_mobnetv2 = mob_netv2.fit(X_train, Y_train, epochs=10, batch_size=32, validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e273e75d-1539-49f2-8d6a-8da0e9f9a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The history.history[\"loss\"] entry is a dictionary with as many values as epochs that the\n",
    "# model was trained on. \n",
    "\n",
    "df_loss_acc = pd.DataFrame(hist_mobnetv2.history)\n",
    "\n",
    "# losses data frame\n",
    "df_loss= df_loss_acc[['loss','val_loss']]\n",
    "df_loss.rename(columns={'loss':'train','val_loss':'validation'},inplace=True)\n",
    "\n",
    "# accuracy data frame\n",
    "df_acc= df_loss_acc[['accuracy','val_accuracy']]\n",
    "df_acc.rename(columns={'accuracy':'train','val_accuracy':'validation'},inplace=True)\n",
    "\n",
    "# plotting the loss and accuracy\n",
    "df_loss.plot(title='Model loss',figsize=(8,6)).set(xlabel='Epoch',ylabel='Loss')\n",
    "df_acc.plot(title='Model Accuracy',figsize=(8,6)).set(xlabel='Epoch',ylabel='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba5f00-6468-429d-a2c0-90eee82b5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_train_mob = mob_netv2.predict(X_train)\n",
    "ypred_train_mob[ypred_train_mob > 0.5] = 1\n",
    "ypred_train_mob[ypred_train_mob < 0.5] = 0\n",
    "\n",
    "ypred_test_mob = mob_netv2.predict(X_test)\n",
    "ypred_test_mob[ypred_test_mob > 0.5] = 1\n",
    "ypred_test_mob[ypred_test_mob < 0.5] = 0\n",
    "\n",
    "print(\"Training data:\")\n",
    "print(f\" Precision : {precision_score(Y_train, ypred_train_mob)}, Recall: {recall_score(Y_train, ypred_train_mob)},F1-score:{f1_score(Y_train, ypred_train_mob)}\")\n",
    "\n",
    "print(\"Test data:\")\n",
    "print(f\" Precision : {precision_score(Y_test, ypred_test_mob)}, Recall: {recall_score(Y_test, ypred_test_mob)}, F1-score:{f1_score(Y_test, ypred_test_mob)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445cab06-54f9-4f91-8c98-e8bcefbdf047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix from the test data\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test, ypred_test_mob, display_labels = ['Paved','Unpaved'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed201d-25cc-49db-94e5-ea65b1dbe487",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, ypred_test_mob, target_names=['Paved', 'Unpaved']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1766fb3b-cccf-4304-a456-c1d819ec2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize some of the results from our modeling efforts\n",
    "fig, axs = plt.subplots(4, 5, figsize=(12,10), tight_layout = True)\n",
    "axs = axs.flatten()\n",
    "for i in range(len(axs)):\n",
    "    axs[i].imshow(X_test[i, :, :, :])\n",
    "    axs[i].set_title(f\"Cat:{ypred_test_mob[i]}\")\n",
    "    axs[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760b72c-d5c7-47a7-bbf3-4c2549bcb566",
   "metadata": {},
   "source": [
    "The models results looks really good by using the image nets. Since we are getting very accurate results with the pretrained imagenet weights, further fine tuning of the outer layers is not necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
